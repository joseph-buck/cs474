{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseph-buck/cs474/blob/main/lab-notebooks/DL_Lab3_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZFaIgEQg1O4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Lab 3: Intro to CNNs and DNNs\n",
        "\n",
        "## Objectives\n",
        "\n",
        "* Build and train a deep convolutional net\n",
        "* Explore and implement various initialization techniques\n",
        "* Implement a parameterized module in Pytorch\n",
        "* Use a principled loss function\n",
        "\n",
        "## Video Tutorial\n",
        "[https://youtu.be/3TAuTcx-VCc](https://youtu.be/3TAuTcx-VCc)\n",
        "\n",
        "## Deliverable\n",
        "For this lab, you will submit an IPython notebook via Learning Suite.\n",
        "This is where you build your first deep neural network!\n",
        "\n",
        "For this lab, we'll be combining several different concepts that we've covered during class,\n",
        "including new layer types, initialization strategies, and an understanding of convolutions.\n",
        "\n",
        "## Grading Standards:\n",
        "* 20% Part 0: Successfully followed lab video and typed in code\n",
        "* 20% Part 1: Re-implement Conv2D and CrossEntropy loss function\n",
        "* 20% Part 2: Implement different initialization strategies\n",
        "* 10% Part 3: Print parameters, plot train/test accuracies\n",
        "* 10% Reach 85% validation accuracy from parts 1-3\n",
        "* 10% Part 4: Convolution parameters quiz\n",
        "* 10% Tidy and legible figures, including labeled axes where appropriate\n",
        "___\n",
        "\n",
        "### Part 0\n",
        "Watch and follow video tutorial:\n",
        "\n",
        "[https://youtu.be/3TAuTcx-VCc](https://youtu.be/3TAuTcx-VCc)\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "* Watch tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "p1247jmKg1PO",
        "outputId": "079b2ab8-7c38-40e7-abeb-b9fea80e04e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "wQOefmcZVgTl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "assert torch.cuda.is_available(), \"You need to request a GPU from Runtime > Change Runtime\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.parameter import Parameter\n",
        "import pdb"
      ],
      "metadata": {
        "id": "Yf2LYtfdh1Yh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "collapsed": true,
        "id": "Il_53HLSWPTY"
      },
      "outputs": [],
      "source": [
        "# Use the dataset class you created in lab2\n",
        "# Create a dataset class that extends the torch.utils.data Dataset class here\n",
        "class FashionMNISTProcessedDataset(Dataset):\n",
        "  def __init__(self, root, train=True):\n",
        "    self.data = datasets.FashionMNIST(root, train=train, \n",
        "                                      transform=transforms.ToTensor(), \n",
        "                                      download=True)\n",
        "  def __getitem__(self, i):\n",
        "    x, y = self.data[i]\n",
        "    return x, y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4owfQwm-Ni"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 1\n",
        "Re-implement a Conv2D module with parameters and a CrossEntropyLoss function. Do not use PyTorch's implementation of cross entropy loss (F.cross_entropy or nn.CrossEntropyLoss) for your re-implementation of cross entropy loss. You may use PyTorch's functional implemenation of 2D convolution in your Conv2D module.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* CrossEntropyLoss \n",
        "* Conv2D\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "___\n",
        "\n",
        "### Part 2\n",
        "Implement three initialization strategies: Xe initialization\n",
        "(sometimes called Xavier), orthogonal initialization, and uniform random initialization.\n",
        "You can specify which strategy you want to use with a parameter.\n",
        "\n",
        "\n",
        "\n",
        "Helpful links include:\n",
        "*  [Orthogonal Initialization](https://hjweide.github.io/orthogonal-initialization-in-convolutional-layers) (or the original paper: http://arxiv.org/abs/1312.6120)\n",
        "*  http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
        "\n",
        "**TODO:**\n",
        "* Parameterize custom Conv2D for different initilization strategies\n",
        "* Xe\n",
        "* Orthogonal\n",
        "* Uniform\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "collapsed": true,
        "id": "RkieTbwlYWPS"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss(nn.Module):\n",
        "  pass\n",
        "\n",
        "class Conv2d(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "    self.__dict__.update(locals())\n",
        "    super(Conv2d, self).__init__()\n",
        "\n",
        "    self.weight = Parameter(torch.Tensor(out_channels, \n",
        "                               in_channels, \n",
        "                               kernel_size[0], \n",
        "                               kernel_size[1]))\n",
        "    self.bias = Parameter(torch.Tensor(out_channels))\n",
        "\n",
        "  # Initialize \n",
        "  def forward(self, x):\n",
        "    return F.conv2d(x, self.weight, self.bias, self.stride, \n",
        "                    self.padding, self.dilation, self.groups)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "collapsed": true,
        "id": "d4C-_v9Hm7YE"
      },
      "outputs": [],
      "source": [
        "class ConvNetwork(nn.Module):\n",
        "  def __init__(self, dataset):\n",
        "    super(ConvNetwork, self).__init__()\n",
        "    x, y = dataset[0]\n",
        "    c, h, w = x.size()\n",
        "    output = 10\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        Conv2d(c, 10, (3, 3), padding=(1, 1)), # c is the number of channels, 10 is the number of kernels, 3x3 is kernel dims, paddings.\n",
        "        Conv2d(10, output, (28, 28), padding=(0, 0)),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x).squeeze(2).squeeze(2) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "collapsed": true,
        "id": "jYqywck8Wdm9",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the datasets\n",
        "train_dataset = FashionMNISTProcessedDataset('/tmp/fashionmnist')\n",
        "val_dataset = FashionMNISTProcessedDataset('/tmp/fashionmnist', train=False)\n",
        "\n",
        "# Initialize the model\n",
        "model = ConvNetwork(train_dataset)\n",
        "model = model.cuda()\n",
        "\n",
        "# Initialize the objective function, the optimizer, and other parameters\n",
        "objective = nn.CrossEntropyLoss() # NOT DEFINED YET, CODE UP YOUR OWN\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Initialize the dataloaders\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=42,\n",
        "                          pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=42,\n",
        "                        pin_memory=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "collapsed": true,
        "id": "mTg1jyIsYVZN",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cebf89-c2de-4314-d163-4acfe60a0241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([42, 1, 28, 28])\n",
            "torch.Size([42])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run your training and validation loop and collect stats\n",
        "losses = []\n",
        "validations = []\n",
        "\n",
        "for epoch in range(1):\n",
        "  loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "  for batch, (x, y_truth) in enumerate(train_loader):\n",
        "    x, y_truth = x.cuda(non_blocking=True), y_truth.cuda(non_blocking=True)\n",
        "\n",
        "    print(x.size())\n",
        "    print(y_truth.size())\n",
        "\n",
        "\n",
        "    #break\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_hat = model(x)\n",
        "\n",
        "    break\n",
        "\n",
        "    loss = objective(y_hat, y_truth)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    accuracy = 0;\n",
        "    loop.set_description('epoch:{}, loss:{:.4f}, accuracy:{:.3f}'.format(epoch, loss, accuracy))\n",
        "    loop.update(1)\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 500 == 0:\n",
        "      val = np.mean([objective(model(x.cuda()), y.cuda()).item() for x, y in val_loader])\n",
        "      validations.append((len(losses), val))\n",
        "    \n",
        "  loop.close()\n",
        "\n",
        "#a, b = zip(*validations)\n",
        "#plt.plot(losses, label='train')\n",
        "#plt.plot(a, b, label='val')\n",
        "#plt.legend()\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ronkEckHiDaU"
      },
      "source": [
        "\n",
        "___\n",
        "\n",
        "### Part 3\n",
        "Print the number of parameters in your network and plot accuracy of your training and validation \n",
        "set over time. You should experiment with some deep networks and see if you can get a network \n",
        "with close to 1,000,000 parameters.\n",
        "\n",
        "Once you've experimented with multiple network setups and the different initialization strategies, plot the best-performing experiments with each initialization strategy. You should be able to exceed 85% accuracy on the validation set in at least one of the plotted experiments.\n",
        "\n",
        "**TODO:**\n",
        "* Experiment with Deep Networks\n",
        "* Plot accuracy of training and validation set over time for each initialization strategy (w/ accurate graph title and axes lables)\n",
        "* Plot experiment results with 85% or better validation accuracy\n",
        "* Print out number of parameters in the model \n",
        "\n",
        "**DONE:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PaWCKjxvyRSf",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Go back up and try a few different networks and initialization strategies\n",
        "# Plot training and validation loss for uniform initialization\n",
        "# Plot training and validation accuracy for uniform initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AM5ikYxg1Pg"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss for Xe initialization\n",
        "# Plot training and validation accuracy for Xe initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpzqC_80g1Pg"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss for orthogonal initialization\n",
        "# Plot training and validation accuracy for orthogonal initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oijCR-JnyS6V",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Compute and print the number of parameters in the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXGRxUQh9gX"
      },
      "source": [
        "___\n",
        "\n",
        "### Part 4\n",
        "Learn about how convolution layers affect the shape of outputs, and answer the following quiz questions. Include these in a new markdown cell in your jupyter notebook.\n",
        "\n",
        "\n",
        "*Using a Kernel size of 3×3 what should the settings of your 2d convolution be that results in the following mappings (first answer given to you)*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(3, 3), padding=(0, 0))\n",
        "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=65, h=12, w=12) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=7, h=20, w=20) : **Your answer in bold here**\n",
        "\n",
        "*Using a Kernel size of 5×5:*)\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(5, 5), padding=(1, 1))\n",
        "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
        "\n",
        "*Using Kernel size of 5×3:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
        "\n",
        "*Determine the kernel that requires the smallest padding size to make the following mappings possible:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=9, w=7) : **Your answer in bold here**\n",
        "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Answer all the questions above \n",
        "\n",
        "**DONE:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XXfG3wClh8an",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Write some test code for checking the answers for these problems (example shown in the video)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL_Lab3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}